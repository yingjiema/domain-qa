{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241f4578-a8d6-4547-8eaf-b8ae6270af55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/dev/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "from haystack.nodes import RAGenerator\n",
    "from haystack.pipelines import GenerativeQAPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9999631f-c9a5-4f87-8e8d-5d8b820e13dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = ElasticsearchDocumentStore(\n",
    "    username='',\n",
    "    password='',\n",
    "    index='bioasq',\n",
    "    # similarity=\"cosine\",\n",
    "    # embedding_dim=768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d1afa5-0944-4cb2-b84b-08aa1affae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"dmis-lab/biobert-base-cased-v1.2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b727326-5ad3-4064-ad27-aaf4ce060866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a48da4e-aa89-443a-901a-651131047cde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:haystack.nodes.answer_generator.transformers:top_k value should not be greater than num_beams, hence setting it to 2\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "Some weights of the model checkpoint at facebook/rag-sequence-nq were not used when initializing RagTokenForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.weight', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RagTokenForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagTokenForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RagTokenForGeneration were not initialized from the model checkpoint at facebook/rag-sequence-nq and are newly initialized: ['rag.generator.lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "generator = RAGenerator(\n",
    "    model_name_or_path=\"facebook/rag-sequence-nq\",\n",
    "    retriever=retriever,\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74f7f6ac-adf4-4593-a704-a4d340e07654",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = GenerativeQAPipeline(generator=generator, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "112dfd44-01d4-43f1-ad34-0d3ad2fc3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30373e60-bcb2-44ab-8ee3-d2e208099b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.26 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.34s/ Batches]\n"
     ]
    }
   ],
   "source": [
    "result = pipeline.run(query='What type of enzyme is peroxiredoxin 2 (PRDX2)?', params={\"Retriever\": {\"top_k\": 20}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "291fd083-0d95-4adb-80cf-a2e8b7d626d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What type of enzyme is peroxiredoxin 2 (PRDX2)?\n",
      "Answers:\n",
      "[{'answer': ' enzymes'}, {'answer': ' enzyme'}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(result, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d3482-9eec-4b78-b724-5cdd221cf634",
   "metadata": {},
   "source": [
    "### Seq2SeqGenerator [WIP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e14499-6839-4acc-93d4-4fc1f5d79c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import Seq2SeqGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "411179ab-7c82-44cb-8162-861ab7f9d84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.17k/1.17k [00:00<00:00, 957kB/s]\n",
      "Downloading spiece.model: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 773k/773k [00:00<00:00, 57.4MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.32M/1.32M [00:00<00:00, 65.3MB/s]\n",
      "/home/ec2-user/anaconda3/envs/dev/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Downloading pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 231M/231M [00:03<00:00, 64.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "generator = Seq2SeqGenerator(\n",
    "    model_name_or_path=\"t5-small\",\n",
    "    input_converter\n",
    "    top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4d34a92-51ca-4548-9f90-48f1d1d93650",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = GenerativeQAPipeline(generator=generator, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcc2bf9c-759f-4a01-814f-b2f98e0a5495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deb7d324-7cde-4d2c-b5e4-0b34dc498418",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.84 Batches/s]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Exception while running node 'Generator': \"Seq2SeqGenerator doesn't have input converter registered for t5-small. Provide custom converter for t5-small in Seq2SeqGenerator initialization\"\nEnable debug logging to see the data that was passed when the pipeline failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.8/site-packages/haystack/pipelines/base.py:521\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, query, file_paths, labels, documents, meta, params, debug)\u001b[0m\n\u001b[1;32m    520\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m` with input: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, node_id, node_input)\n\u001b[0;32m--> 521\u001b[0m     node_output, stream_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;66;03m# The input might be a really large object with thousands of embeddings.\u001b[39;00m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;66;03m# If you really want to see it, raise the log level.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.8/site-packages/haystack/pipelines/base.py:447\u001b[0m, in \u001b[0;36mPipeline._run_node\u001b[0;34m(self, node_id, node_input)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_id: \u001b[38;5;28mstr\u001b[39m, node_input: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnode_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.8/site-packages/haystack/nodes/base.py:201\u001b[0m, in \u001b[0;36mBaseComponent._dispatch_run\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03mThe Pipelines call this method when run() is executed. This method in turn executes the _dispatch_run_general()\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03mmethod with the correct run method.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_run_general\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.8/site-packages/haystack/nodes/base.py:245\u001b[0m, in \u001b[0;36mBaseComponent._dispatch_run_general\u001b[0;34m(self, run_method, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m         run_inputs[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 245\u001b[0m output, stream \u001b[38;5;241m=\u001b[39m \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrun_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrun_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Collect debug information\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.8/site-packages/haystack/nodes/answer_generator/base.py:37\u001b[0m, in \u001b[0;36mBaseGenerator.run\u001b[0;34m(self, query, documents, top_k, labels, add_isolated_node_eval)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m documents:\n\u001b[0;32m---> 37\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.8/site-packages/haystack/nodes/answer_generator/transformers.py:441\u001b[0m, in \u001b[0;36mSeq2SeqGenerator.predict\u001b[0;34m(self, query, documents, top_k)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m converter:\n\u001b[0;32m--> 441\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeq2SeqGenerator doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have input converter registered for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvide custom converter for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in Seq2SeqGenerator initialization\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Seq2SeqGenerator doesn't have input converter registered for t5-small. Provide custom converter for t5-small in Seq2SeqGenerator initialization\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWhich gene is most commonly associated with severe congenital and cyclic neutropenia?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRetriever\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.8/site-packages/haystack/pipelines/standard_pipelines.py:450\u001b[0m, in \u001b[0;36mGenerativeQAPipeline.run\u001b[0;34m(self, query, params, debug)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, params: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, debug: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    :param query: the query string.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    :param params: params for the `retriever` and `generator`. For instance,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m          by this method under the key \"_debug\"\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.8/site-packages/haystack/utils/reflection.py:32\u001b[0m, in \u001b[0;36mpipeline_invocation_counter.<locals>.wrapper_invocation_counter\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         this_invocation_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     31\u001b[0m wrapper_invocation_counter\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m this_invocation_count\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.8/site-packages/haystack/pipelines/base.py:526\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, query, file_paths, labels, documents, meta, params, debug)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;66;03m# The input might be a really large object with thousands of embeddings.\u001b[39;00m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;66;03m# If you really want to see it, raise the log level.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while running node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with input \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, node_id, node_input)\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while running node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEnable debug logging to see the data that was passed when the pipeline failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    529\u001b[0m queue\u001b[38;5;241m.\u001b[39mpop(node_id)\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Exception while running node 'Generator': \"Seq2SeqGenerator doesn't have input converter registered for t5-small. Provide custom converter for t5-small in Seq2SeqGenerator initialization\"\nEnable debug logging to see the data that was passed when the pipeline failed."
     ]
    }
   ],
   "source": [
    "result = pipeline.run(query='Which gene is most commonly associated with severe congenital and cyclic neutropenia?', params={\"Retriever\": {\"top_k\": 10}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f914977-9450-47ba-a2c2-5477ec7f3feb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
