{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2cc8fb-b7d3-40e0-a940-ec83af011085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/dev/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.nodes import EmbeddingRetriever, TfidfRetriever\n",
    "from haystack.nodes import FARMReader\n",
    "from haystack.pipelines import ExtractiveQAPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee0e1a0-3d93-4984-aff5-ef9336ca30b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = ElasticsearchDocumentStore(\n",
    "    host='3.83.13.28',\n",
    "    username='',\n",
    "    password='',\n",
    "    index='bioasq',\n",
    "    # similarity=\"cosine\",\n",
    "    # embedding_dim=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95f4eec7-20d5-4b8c-a5db-dbc13b2b6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"dmis-lab/biobert-base-cased-v1.2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "440b9b7c-9f6e-44d9-9dd0-6eea83d71116",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating embeddings:   0%|                                                                                                                                                                      | 0/1490 [00:00<?, ? Docs/s]\n",
      "Inferencing Samples:   0%|                                                                                                                                                                     | 0/47 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:   2%|███▎                                                                                                                                                         | 1/47 [00:18<14:22, 18.75s/ Batches]\u001b[A\n",
      "Inferencing Samples:   4%|██████▋                                                                                                                                                      | 2/47 [00:37<14:13, 18.98s/ Batches]\u001b[A\n",
      "Inferencing Samples:   6%|██████████                                                                                                                                                   | 3/47 [00:56<13:50, 18.88s/ Batches]\u001b[A\n",
      "Inferencing Samples:   9%|█████████████▎                                                                                                                                               | 4/47 [01:15<13:31, 18.87s/ Batches]\u001b[A\n",
      "Inferencing Samples:  11%|████████████████▋                                                                                                                                            | 5/47 [01:34<13:12, 18.86s/ Batches]\u001b[A\n",
      "Inferencing Samples:  13%|████████████████████                                                                                                                                         | 6/47 [01:53<12:53, 18.88s/ Batches]\u001b[A\n",
      "Inferencing Samples:  15%|███████████████████████▍                                                                                                                                     | 7/47 [02:11<12:33, 18.83s/ Batches]\u001b[A\n",
      "Inferencing Samples:  17%|██████████████████████████▋                                                                                                                                  | 8/47 [02:30<12:13, 18.81s/ Batches]\u001b[A\n",
      "Inferencing Samples:  19%|██████████████████████████████                                                                                                                               | 9/47 [02:49<11:53, 18.78s/ Batches]\u001b[A\n",
      "Inferencing Samples:  21%|█████████████████████████████████▏                                                                                                                          | 10/47 [03:08<11:34, 18.78s/ Batches]\u001b[A\n",
      "Inferencing Samples:  23%|████████████████████████████████████▌                                                                                                                       | 11/47 [03:27<11:16, 18.78s/ Batches]\u001b[A\n",
      "Inferencing Samples:  26%|███████████████████████████████████████▊                                                                                                                    | 12/47 [03:45<10:57, 18.78s/ Batches]\u001b[A\n",
      "Inferencing Samples:  28%|███████████████████████████████████████████▏                                                                                                                | 13/47 [04:04<10:38, 18.79s/ Batches]\u001b[A\n",
      "Inferencing Samples:  30%|██████████████████████████████████████████████▍                                                                                                             | 14/47 [04:23<10:19, 18.77s/ Batches]\u001b[A\n",
      "Inferencing Samples:  32%|█████████████████████████████████████████████████▊                                                                                                          | 15/47 [04:42<10:00, 18.78s/ Batches]\u001b[A\n",
      "Inferencing Samples:  34%|█████████████████████████████████████████████████████                                                                                                       | 16/47 [05:00<09:41, 18.77s/ Batches]\u001b[A\n",
      "Inferencing Samples:  36%|████████████████████████████████████████████████████████▍                                                                                                   | 17/47 [05:19<09:23, 18.77s/ Batches]\u001b[A\n",
      "Inferencing Samples:  38%|███████████████████████████████████████████████████████████▋                                                                                                | 18/47 [05:38<09:04, 18.77s/ Batches]\u001b[A\n",
      "Inferencing Samples:  40%|███████████████████████████████████████████████████████████████                                                                                             | 19/47 [05:57<08:45, 18.77s/ Batches]\u001b[A\n",
      "Inferencing Samples:  43%|██████████████████████████████████████████████████████████████████▍                                                                                         | 20/47 [06:15<08:26, 18.77s/ Batches]\u001b[A\n",
      "Inferencing Samples:  45%|█████████████████████████████████████████████████████████████████████▋                                                                                      | 21/47 [06:34<08:07, 18.75s/ Batches]\u001b[A\n",
      "Inferencing Samples:  47%|█████████████████████████████████████████████████████████████████████████                                                                                   | 22/47 [06:53<07:48, 18.75s/ Batches]\u001b[A\n",
      "Inferencing Samples:  49%|████████████████████████████████████████████████████████████████████████████▎                                                                               | 23/47 [07:12<07:30, 18.75s/ Batches]\u001b[A\n",
      "Inferencing Samples:  51%|███████████████████████████████████████████████████████████████████████████████▋                                                                            | 24/47 [07:30<07:11, 18.75s/ Batches]\u001b[A\n",
      "Inferencing Samples:  53%|██████████████████████████████████████████████████████████████████████████████████▉                                                                         | 25/47 [07:49<06:52, 18.75s/ Batches]\u001b[A\n",
      "Inferencing Samples:  55%|██████████████████████████████████████████████████████████████████████████████████████▎                                                                     | 26/47 [08:08<06:33, 18.75s/ Batches]\u001b[A\n",
      "Inferencing Samples:  57%|█████████████████████████████████████████████████████████████████████████████████████████▌                                                                  | 27/47 [08:27<06:14, 18.75s/ Batches]\u001b[A\n",
      "Inferencing Samples:  60%|████████████████████████████████████████████████████████████████████████████████████████████▉                                                               | 28/47 [08:45<05:56, 18.74s/ Batches]\u001b[A\n",
      "Inferencing Samples:  62%|████████████████████████████████████████████████████████████████████████████████████████████████▎                                                           | 29/47 [09:04<05:37, 18.75s/ Batches]\u001b[A\n",
      "Inferencing Samples:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████▌                                                        | 30/47 [09:23<05:18, 18.75s/ Batches]\u001b[A\n",
      "Inferencing Samples:  66%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                     | 31/47 [09:42<04:59, 18.74s/ Batches]\u001b[A\n",
      "Inferencing Samples:  68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                 | 32/47 [10:00<04:41, 18.73s/ Batches]\u001b[A\n",
      "Inferencing Samples:  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                              | 33/47 [10:19<04:22, 18.73s/ Batches]\u001b[A\n",
      "Inferencing Samples:  72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                           | 34/47 [10:38<04:03, 18.73s/ Batches]\u001b[A\n",
      "Inferencing Samples:  74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                       | 35/47 [10:57<03:45, 18.76s/ Batches]\u001b[A\n",
      "Inferencing Samples:  77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                    | 36/47 [11:16<03:26, 18.80s/ Batches]\u001b[A\n",
      "Inferencing Samples:  79%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                 | 37/47 [11:34<03:08, 18.80s/ Batches]\u001b[A\n",
      "Inferencing Samples:  81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                             | 38/47 [11:53<02:49, 18.80s/ Batches]\u001b[A\n",
      "Inferencing Samples:  83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                          | 39/47 [12:12<02:30, 18.78s/ Batches]\u001b[A\n",
      "Inferencing Samples:  85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                       | 40/47 [12:31<02:11, 18.77s/ Batches]\u001b[A\n",
      "Inferencing Samples:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                    | 41/47 [12:49<01:52, 18.76s/ Batches]\u001b[A\n",
      "Inferencing Samples:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                | 42/47 [13:08<01:33, 18.78s/ Batches]\u001b[A\n",
      "Inferencing Samples:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋             | 43/47 [13:27<01:15, 18.84s/ Batches]\u001b[A\n",
      "Inferencing Samples:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████          | 44/47 [13:46<00:56, 18.81s/ Batches]\u001b[A\n",
      "Inferencing Samples:  96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎      | 45/47 [14:05<00:37, 18.80s/ Batches]\u001b[A\n",
      "Inferencing Samples:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 46/47 [14:23<00:18, 18.78s/ Batches]\u001b[A\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47/47 [14:33<00:00, 18.59s/ Batches]\u001b[A\n",
      "Updating embeddings: 10000 Docs [14:42, 11.33 Docs/s]                                                                                                                                                                       \n"
     ]
    }
   ],
   "source": [
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d501152-27aa-49e8-a75c-ad53a93c8dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 477/477 [00:00<00:00, 500kB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 127M/127M [00:02<00:00, 63.8MB/s]\n",
      "Downloading tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 107/107 [00:00<00:00, 95.1kB/s]\n",
      "Downloading vocab.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 226k/226k [00:00<00:00, 37.4MB/s]\n",
      "Downloading special_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 120kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "reader = FARMReader(model_name_or_path=\"deepset/minilm-uncased-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b37a856a-ffff-4401-bdb5-49463e9b42f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ac704b0-7b10-4c04-8044-503259fb8131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.21 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.42 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.57 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.75 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.33 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.30 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.39 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.56 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.39 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.81 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.34 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.38 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.24 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.52 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.73 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.66 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.80 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.35 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.45 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.17 Batches/s]\n",
      "Inferencing Samples: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.80 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = pipe.run(\n",
    "    query='What type of enzyme is peroxiredoxin 2 (PRDX2)?',\n",
    "    params={\"Retriever\": {\"top_k\": 20}, \"Reader\": {\"top_k\": 5}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be0cf54c-7e20-4b34-89d2-199ad0f5efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed2476d7-e6ca-46af-99fa-6f013589951d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What type of enzyme is peroxiredoxin 2 (PRDX2)?\n",
      "Answers:\n",
      "[   {   'answer': 'antioxidant',\n",
      "        'context': 'Peroxiredoxin 2 (Prx2) is an antioxidant enzyme that uses '\n",
      "                   'cysteine residues to decompose peroxides. Prx2 is the '\n",
      "                   'third most abundant protein in erythro'},\n",
      "    {   'answer': 'SGLT2',\n",
      "        'context': 'Empagliflozin (Jardiance): a novel SGLT2 inhibitor for the '\n",
      "                   'treatment of type-2 diabetes.'},\n",
      "    {   'answer': 'Transcription factor',\n",
      "        'context': 'Transcription factor forkhead box protein P2 (FOXP2) plays '\n",
      "                   'an essential role in the development of language and '\n",
      "                   'speech. However, the transcriptional a'},\n",
      "    {   'answer': 'calpromotin',\n",
      "        'context': 'ytic anemia associated with Heinz body formation. Prx2, '\n",
      "                   'also known as calpromotin, regulates ion transport by '\n",
      "                   'associating with the membrane and activa'},\n",
      "    {   'answer': 'alpha-galactosidase',\n",
      "        'context': 'Transgenic mice expressing a human mutant '\n",
      "                   'alpha-galactosidase with an R301Q substitution, which was '\n",
      "                   'found in a patient with a variant form of Fabry di'}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c54a5b-397d-4c87-b258-d25ef9e1ac8d",
   "metadata": {},
   "source": [
    "The first answer is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd872a-64b5-426b-b195-2b18835a8e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
